# AGHMN
Implementation of the paper [Real-Time Emotion Recognition via Attention GatedHierarchical Memory Network](https://aaai.org/ojs/index.php/AAAI/article/view/6309) in AAAI-2020.

(The scripts uploaded here need further clean, please wait.)

## 1. Dataset

Please find the datasets:
- [IEMOCAP](https://sail.usc.edu/iemocap/): IEMOCAP contains approximately 12 hours of audiovisual data, including video, speech, motion capture of face, text transcriptions.
- [MELD](https://github.com/SenticNet/MELD): A multimodal multi-party dataset for emotion recognition in conversation. 

## 2. Run 

### Prerequisites
- Python v3.6
- Pytorch v0.4.0-v0.4.1
- Pickle

# 3. Citation
Please kindly cite our paper:
```ruby
@article{jiao2019real,
  title={Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network},
  author={Jiao, Wenxiang and Lyu, Michael R and King, Irwin},
  journal={arXiv preprint arXiv:1911.09075},
  year={2019}
}
```
